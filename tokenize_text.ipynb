{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a9422352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure you pip install\n",
    "#TODO: N-grams \n",
    "#Stop word removal and tokenization\n",
    "#Add any extra text cleaning wanted\n",
    "#Save to the CSV when all checked so that people can do work in other files\n",
    "#Currently only works with the cleaned dataset. Lots more work needed for other one\n",
    "#Get rid of other languages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "#Replace with your own path\n",
    "f = open(\"../data/HateSpeechDataset.csv\",'r')\n",
    "r_cols = ['tweet', 'hate', 'nums']\n",
    "tweets = pd.read_csv(f, sep=',', names=r_cols)\n",
    "tweets = tweets[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ec03ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = dict()\n",
    "word_counter = 0\n",
    "\n",
    "#This is only in english, not sure how many languages are in there...\n",
    "stop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \n",
    "              \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \n",
    "              \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \n",
    "              \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \n",
    "              \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \n",
    "              \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \n",
    "              \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\",\n",
    "              \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \n",
    "              \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \n",
    "              \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \n",
    "              \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \n",
    "              \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \n",
    "              \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c74f3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Couple potential additions: \n",
    "#Add other domains like .info, .uk, .de etc. There are a couple instances in data where would be nice\n",
    "#Hyper text transfer protocol seems to Start many of these urls. Probably shouldnt matter because we use naive bayes\n",
    "#Also, like iloc 532 shows, there are other url components that this doesnt remove, but i dont want to kill the whole tweet\n",
    "#Any ideas to fix this are appreciated\n",
    "\n",
    "#Currently just removes urls\n",
    "def clean_text(text):\n",
    "    clean_tweet = text + \" \"\n",
    "    clean_tweet = re.sub(\" http| https\", \"\", clean_tweet)\n",
    "\n",
    "    clean_tweet = re.sub(\"hyper text transfer protocol\", \"\", clean_tweet)\n",
    "\n",
    "    clean_tweet = re.sub(\"www .* com \", \"\", clean_tweet)\n",
    "    clean_tweet = re.sub(\"www .* org \", \"\", clean_tweet)\n",
    "    clean_tweet = re.sub(\"www .* net \", \"\", clean_tweet)\n",
    "    clean_tweet = re.sub(\"www .* uk \", \"\", clean_tweet)\n",
    "\n",
    "    clean_tweet = re.sub(\" $\", \"\", clean_tweet)\n",
    "    return clean_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d45f169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uses hash table to go through the tweets and assign numbers pretty fast\n",
    "#Honestly pretty simple, should do with and without stop words for testing\n",
    "\n",
    "def number_words(text):\n",
    "    #Im being told by online people it is bad design to use global variables\n",
    "    #Feel free to fix as wanted\n",
    "    global word_dict\n",
    "    global word_counter\n",
    "    new_numbers = []\n",
    "    for i in text:\n",
    "        if i in word_dict:\n",
    "            new_numbers.append(word_dict[i])\n",
    "        else:\n",
    "            word_dict[i] = word_counter\n",
    "            word_counter = word_counter + 1\n",
    "            new_numbers.append(word_dict[i])\n",
    "    return new_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a01db7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Efficiency will be added. JAMES, you must think...\n",
    "#perhaps another hash table.\n",
    "\n",
    "def stop_word_removal(text):\n",
    "    return_text = []\n",
    "    global stop_words\n",
    "    for i in range(len(text)):\n",
    "        if not text[i] in stop_words:\n",
    "            return_text.append(text[i])\n",
    "            \n",
    "    return return_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f7ea63df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are all split up so you can test whatever you want to do\n",
    "tweets['tweet'] = tweets['tweet'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b2efdef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['splits'] = tweets['tweet'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b6e00cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['newNum'] = tweets['splits'].apply(number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dcb93d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This takes a while\n",
    "tweets['noStop'] = tweets['splits'].apply(stop_word_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "53faf91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hashtag print any collumn you want :) these functions should be used with each other and can help out the old data visualization too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "51884fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv(\"finished_hate_speech.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
